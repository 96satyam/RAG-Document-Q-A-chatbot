# ğŸš€ AI-Powered Research Paper Q&A with RAG, Groq & Mixtral
### ğŸ” Overview
This project implements a Retrieval-Augmented Generation (RAG) pipeline using FAISS, Hugging Face embeddings, and Groqâ€™s Mixtral-8x7B to enable efficient, context-aware question-answering on research papers. It provides an interactive Streamlit UI for querying PDFs and retrieving factually grounded responses.

### âš™ï¸ Architecture
1ï¸âƒ£ Document Processing & Vectorization
ğŸ“‚ PDF Loader: Extracts text from research papers (PyPDFDirectoryLoader).
ğŸ§© Text Splitting: Uses RecursiveCharacterTextSplitter (1k tokens, 200 overlap) for optimal chunking.
ğŸ§  Embedding Generation: Converts text into dense vectors using sentence-transformers/all-MiniLM-L6-v2.
ğŸ“Œ FAISS Indexing: Stores embeddings for fast nearest-neighbor retrieval.
2ï¸âƒ£ Retrieval-Augmented Generation (RAG) Pipeline
ğŸ” Query Processing: Converts user input into vector space.
ğŸ“‘ Context Retrieval: Matches against FAISS vectors to fetch top-k relevant chunks.
ğŸ¤– LLM-Based Answering: Feeds retrieved context into Mixtral-8x7B via LangChain for precise, context-grounded responses.
3ï¸âƒ£ Interactive UI (Streamlit)
âš¡ Real-time Q&A with dynamic document embedding.
ğŸ“Œ Similarity Search Visualization for transparency.
â³ Response Time Tracking using Pythonâ€™s time.process_time().
ğŸ”— Tech Stack
âœ… Python | Streamlit | LangChain | FAISS | Hugging Face | Groq API

### ğŸš€ Future Enhancements
ğŸ”¹ Hybrid Search (BM25 + Dense Retrieval) for better relevance.
ğŸ”¹ Multi-turn Conversations with memory augmentation.
ğŸ”¹ Fine-tuned LLMs for specialized research domains.
ğŸ’¡ Why It Matters
Traditional keyword-based search lacks semantic depth. This RAG system bridges the gap by ensuring factually accurate, explainable, and efficient information retrieval from research papers.
